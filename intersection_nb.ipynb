{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6218f67d-8e85-4ac8-b80e-909f64791842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fralberti/.local/lib/python3.8/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import xml.etree.ElementTree as xml\n",
    "import pandas as pd\n",
    "from nilearn.plotting import plot_surf_stat_map\n",
    "from matplotlib import pyplot as plt\n",
    "import ciftools_af as ct\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def extract_zone_convergence(sub, hemi, brain_structure):\n",
    "    # load surf\n",
    "    surf_raw = nib.load(f'{root_dir}{sub}/Structural/{sub}.{hemi}.midthickness_MSMAll.32k_fs_LR.surf.gii')\n",
    "    surf = []\n",
    "    surf.append(surf_raw.darrays[0].data)\n",
    "    surf.append(surf_raw.darrays[1].data)\n",
    "    vertices, triangles = surf#[surf_raw.darrays[i].data for i in range(2)]\n",
    "\n",
    "    # load labels\n",
    "    labels = nib.load(f'{root_dir}{sub}/Structural/{sub}.zone_prim.32k_fs_LR.dlabel.nii')\n",
    "    zones = labels.get_fdata().squeeze()\n",
    "\n",
    "    brain_model = [x for x in labels.header.get_index_map(1).brain_models if x.brain_structure==brain_structure][0]\n",
    "    offset = brain_model.index_offset\n",
    "    cortex = np.asarray(brain_model.vertex_indices[0:])\n",
    "    \n",
    "\n",
    "    z = np.zeros(vertices.shape[0])\n",
    "    z[cortex] = zones[offset:offset+len(cortex)]\n",
    "    # next step takes advantage of prod of 1,2,3 == 6.\n",
    "    coords = np.argwhere(np.prod(z[triangles], axis=1) == 6.)\n",
    "\n",
    "    # take more posterior node:\n",
    "    trig_of_interest = np.argmin([vertices[triangles[coords[0]]][0][:,1].mean(), vertices[triangles[coords[1]]][0][:,1].mean()])\n",
    "    nodes_of_interest = triangles[coords[trig_of_interest]][0]\n",
    "    \n",
    "    return nodes_of_interest\n",
    "\n",
    "\n",
    "### Find the scalar value associated to a set of vetices and a their percentile in a specified structure\n",
    "def get_scalar_pctile(cifti_scalar, vertices, brain_structure, scalar_row=0):\n",
    "    \n",
    "    # extract scalar values of all vertices, and the features of the brain structure from cifti\n",
    "    all_scalars = np.array(cifti_scalar.get_fdata()[scalar_row])\n",
    "    brain_model = [x for x in cifti_scalar.header.get_index_map(1).brain_models if x.brain_structure==brain_structure]\n",
    "    offset = brain_model[0].index_offset\n",
    "    count = brain_model[0].index_count\n",
    "    vertex_indices = np.array(brain_model[0].vertex_indices)\n",
    "    idx = np.array([i for i,x in enumerate(vertex_indices) if x in vertices])\n",
    "    \n",
    "    # get scalars and relative percentile\n",
    "    vertex_scalars = all_scalars[offset+idx]\n",
    "    scalar_pctiles = [stats.percentileofscore(all_scalars[offset:offset+count],scalar) for scalar in vertex_scalars]\n",
    "    del cifti_scalar\n",
    "    \n",
    "    return vertex_scalars, scalar_pctiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a4224c-579c-4d22-a12d-baf9f12dea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VARIABLES TO SET BEFORE RUNNING\n",
    "# directory containing subdirectories named fter subject IDs that contain the timeseries and surface files\n",
    "root_dir = \"/home/fralberti/Data/HCP/\"\n",
    "# directory where all intermediate files and the final output will be saved\n",
    "output_dir = \"/home/fralberti/Data/Output_misc/\"\n",
    "# list of IDs of subjects to include in the analyses\n",
    "f = open(f'{root_dir}subj_IDs_200.txt', 'r')\n",
    "subj_id = np.array(f.read().splitlines())\n",
    "del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f287ed-08bb-4320-aad7-844ce029cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting principal gradient and percentile of convergence nodes, and performing permutation \n",
      "Current subject:\n",
      "\t100206\n",
      "\t100307\n",
      "\t100408\n",
      "\t101006\n",
      "\t101107\n",
      "\t101309\n",
      "\t101915\n",
      "\t102008\n",
      "\t102109\n",
      "\t102311\n",
      "\t102513\n",
      "\t102614\n",
      "\t102715\n",
      "\t102816\n",
      "\t103010\n",
      "\t103111\n",
      "\t103212\n",
      "\t103414\n",
      "\t103515\n",
      "\t103818\n",
      "\t104416\n",
      "\t104820\n",
      "\t105014\n",
      "\t105115\n",
      "\t105216\n",
      "\t105620\n",
      "\t105923\n",
      "\t106016\n",
      "\t106521\n",
      "\t106824\n",
      "\t107018\n",
      "\t107321\n",
      "\t107422\n",
      "\t107725\n",
      "\t108020\n",
      "\t108121\n",
      "\t108222\n",
      "\t108323\n",
      "\t108525\n",
      "\t108828\n",
      "\t109123\n",
      "\t109325\n",
      "\t109830\n",
      "\t110007\n",
      "\t110411\n",
      "\t111211\n",
      "\t111312\n",
      "\t111413\n",
      "\t111514\n",
      "\t111716\n",
      "\t112314\n",
      "\t112516\n",
      "\t112920\n",
      "\t113215\n",
      "\t113316\n",
      "\t113619\n",
      "\t114217\n",
      "\t114419\n",
      "\t114621\n",
      "\t114823\n",
      "\t114924\n",
      "\t115017\n",
      "\t115219\n",
      "\t115320\n",
      "\t115724\n",
      "\t115825\n",
      "\t116524\n",
      "\t117021\n",
      "\t117122\n",
      "\t117324\n",
      "\t117930\n",
      "\t118023\n",
      "\t118124\n",
      "\t118225\n",
      "\t118528\n",
      "\t118831\n",
      "\t118932\n",
      "\t119025\n",
      "\t119126\n",
      "\t120111\n",
      "\t120212\n",
      "\t120414\n",
      "\t120515\n",
      "\t120717\n",
      "\t121416\n",
      "\t121618\n",
      "\t122317\n",
      "\t122620\n",
      "\t122822\n",
      "\t123117\n",
      "\t123420\n",
      "\t123521\n",
      "\t123723\n",
      "\t123824\n",
      "\t123925\n",
      "\t124220\n",
      "\t124422\n",
      "\t124624\n",
      "\t124826\n",
      "\t125222\n",
      "\t125424\n",
      "\t125525\n",
      "\t126426\n",
      "\t127226\n",
      "\t127327\n",
      "\t127630\n",
      "\t127731\n",
      "\t127832\n",
      "\t127933\n",
      "\t128026\n",
      "\t128127\n",
      "\t128632\n",
      "\t128935\n",
      "\t129028\n",
      "\t129331\n",
      "\t168947\n",
      "\t169040\n",
      "\t169343\n",
      "\t169444\n",
      "\t169545\n",
      "\t170631\n",
      "\t171330\n",
      "\t171532\n",
      "\t171633\n",
      "\t172029\n",
      "\t172130\n",
      "\t172332\n",
      "\t172433\n",
      "\t172534\n",
      "\t172938\n",
      "\t173334\n",
      "\t173435\n",
      "\t173536\n",
      "\t173738\n",
      "\t173839\n",
      "\t173940\n",
      "\t174437\n",
      "\t174841\n",
      "\t175035\n",
      "\t175136\n",
      "\t175237\n",
      "\t175338\n",
      "\t175439\n",
      "\t175540\n",
      "\t175742\n",
      "\t176037\n",
      "\t176239\n",
      "\t176441\n",
      "\t176542\n",
      "\t176744\n",
      "\t176845\n",
      "\t177140\n",
      "\t177241\n",
      "\t177645\n",
      "\t177746\n",
      "\t178142\n",
      "\t178243\n",
      "\t178647\n",
      "\t178748\n",
      "\t178849\n",
      "\t178950\n",
      "\t179245\n",
      "\t179346\n",
      "\t180129\n",
      "\t180230\n",
      "\t180533\n",
      "\t180735\n",
      "\t180836\n",
      "\t180937\n",
      "\t181131\n",
      "\t181232\n",
      "\t181636\n",
      "\t182032\n",
      "\t182436\n",
      "\t182739\n",
      "\t182840\n",
      "\t183034\n",
      "\t185038\n",
      "\t185341\n",
      "\t185442\n",
      "\t185846\n",
      "\t186040\n",
      "\t186141\n",
      "\t186444\n",
      "\t186545\n",
      "\t186848\n",
      "\t187143\n",
      "\t187850\n",
      "\t188145\n",
      "\t188347\n",
      "\t188448\n",
      "\t188549\n",
      "\t188751\n",
      "\t189349\n",
      "\t189450\n",
      "\t189652\n",
      "\t190031\n",
      "\t191033\n",
      "\t191235\n",
      "\t191336\n",
      "\t191336\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### Obtain the gradient values corresponding to the convergence nodes and relative percentile\n",
    "print(\"Extracting principal gradient and percentile of convergence nodes, and performing permutation \\nCurrent subject:\")\n",
    "\n",
    "# pre-assign the output dataframe\n",
    "gradientile_df = pd.DataFrame(columns=['ID_vtx','ID_grad','hemisphere',\n",
    "                                         'vertex1','vertex2','vertex3',\n",
    "                                         'mean','percentile'])\n",
    "for subj_grad in subj_id:\n",
    "    print(f'\\t{subj_grad}')\n",
    "    # load a subject's gradient1\n",
    "    grads = nib.load(f'{root_dir}{subj_grad}/{subj_grad}.gcca_200.32k_fs_LR.dscalar.nii')\n",
    "    zones = nib.load(f'{root_dir}/HCP_S1200_GroupAvg_v1/zones.watershed.dlabel.nii').get_fdata()\n",
    "    # get gradient and percentile of convergence nodes of all subjs from the current grad       \n",
    "    mask_LR = []\n",
    "    \n",
    "    for hemi in ['L','R']:           \n",
    "        # extract hemisphere's gradient\n",
    "        brain_structure = ['CIFTI_STRUCTURE_CORTEX_LEFT' if hemi=='L' else 'CIFTI_STRUCTURE_CORTEX_RIGHT'][0]       \n",
    "        offset, count, vertex_indices = ct.struct_info(brain_structure, grads)\n",
    "        # apply lateral parietal mask\n",
    "        z = [2 if hemi=='L' else 3][0]\n",
    "        zone_hemi = zones[20, offset:offset+count]\n",
    "        mask = zone_hemi == z\n",
    "        mask_LR.extend(mask)\n",
    "        grad_zone = grads.get_fdata()[0, offset:offset+count][mask]\n",
    "\n",
    "        for subj_vtx in subj_id: \n",
    "            # extract principal gradient position of intersection vtx\n",
    "            vtx_of_interest = extract_zone_convergence(subj_vtx, hemi, brain_structure)\n",
    "            vtx_of_interest = np.sort(vtx_of_interest.tolist(),axis=0)\n",
    "            vtx_grad = np.median(grad_zone[np.isin(vertex_indices[mask],vtx_of_interest)])\n",
    "            vtx_pctile = stats.percentileofscore(grad_zone, vtx_grad)\n",
    "                        \n",
    "            # update output dataframe\n",
    "            gradientile_df = gradientile_df.append({'ID_vtx':subj_vtx,'ID_grad':subj_grad,'hemisphere':hemi,\n",
    "                                                    'vertex1':vtx_of_interest[0],'vertex2':vtx_of_interest[1],'vertex3':vtx_of_interest[2],\n",
    "                                                    'mean':vtx_grad,'percentile':vtx_pctile}, ignore_index=True)\n",
    "\n",
    "    ct.save_dscalar(np.array(mask_LR,ndmin=2), grads, f'{root_dir}{subj_grad}/Structural/{subj_grad}.zone_2.32k_fs_LR.dscalar.nii')\n",
    "\n",
    "\n",
    "del grads\n",
    "# save output        \n",
    "gradientile_df.to_csv(f'{output_dir}gradientiles_s6.csv',index=False)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a4dbbd6-5c01-46e4-8b39-3fd480e0e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH:  WilcoxonResult(statistic=9058.5, pvalue=0.3265831903993619) \n",
      " RH:  WilcoxonResult(statistic=8900.5, pvalue=0.1607395062160054)\n"
     ]
    }
   ],
   "source": [
    "# Perform simple permutation test\n",
    "gradientile_df = pd.read_csv(f'{output_dir}gradientiles_s6.csv')\n",
    "X = gradientile_df[gradientile_df.ID_vtx==gradientile_df.ID_grad].set_index('ID_grad')\n",
    "Y = gradientile_df[gradientile_df.ID_vtx!=gradientile_df.ID_grad].groupby(['ID_grad','hemisphere']).agg('median').reset_index(level=1)\n",
    "L_wsrt = stats.wilcoxon(X.loc[X.hemisphere=='L','percentile'],Y.loc[X.hemisphere=='L','percentile'])\n",
    "R_wsrt = stats.wilcoxon(X.loc[X.hemisphere=='R','percentile'],Y.loc[X.hemisphere=='R','percentile'])\n",
    "print('LH: ', L_wsrt,'\\n','RH: ', R_wsrt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
