{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e500b2ad-1f01-41ee-9036-d40239848b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import mvlearn\n",
    "from mvlearn import embed\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# Load the time series of the L and R hemisphere \n",
    "def load_dts(dtseries_path):\n",
    "    dtseries = nib.load(dtseries_path)\n",
    "    \n",
    "    L_model = [x for x in dtseries.header.get_index_map(1).brain_models if x.brain_structure=='CIFTI_STRUCTURE_CORTEX_LEFT']\n",
    "    R_model = [x for x in dtseries.header.get_index_map(1).brain_models if x.brain_structure=='CIFTI_STRUCTURE_CORTEX_RIGHT']\n",
    "    offset_count = [L_model[0].index_offset, L_model[0].index_count,\n",
    "                    R_model[0].index_offset, R_model[0].index_count]\n",
    "    values = dtseries.get_fdata()\n",
    "    values = values[0:,np.append(np.arange(offset_count[0],offset_count[0]+offset_count[1]),np.arange(offset_count[2],offset_count[2]+offset_count[3]))]\n",
    "    return values\n",
    "\n",
    "\n",
    "# Z-score and conatenation of dtseries of all subject folders in a directory\n",
    "def concat_dts(input_dir, output_dir):\n",
    "    sbj_dirs = next(os.walk(input_dir))[1]\n",
    "    sbj_IDs = []\n",
    "    for sbj in sbj_dirs:\n",
    "        print('searching folder:' + sbj)\n",
    "        sbj_tmp = input_dir + sbj\n",
    "        \n",
    "        files = []\n",
    "        try:\n",
    "            for root, dirnames, filenames in os.walk(sbj_tmp):\n",
    "                for file in filenames:\n",
    "                    files.append(os.path.join(root, file))\n",
    "\n",
    "            dts_lr_path = next((x for x in files if x.endswith('rfMRI_REST1_LR_Atlas_MSMAll.dtseries.nii')), None)\n",
    "            dts_rl_path = next((x for x in files if x.endswith('rfMRI_REST1_RL_Atlas_MSMAll.dtseries.nii')), None)\n",
    "            dts_lr = stats.zscore(load_dts(dts_lr_path), axis=0, nan_policy='omit')\n",
    "            dts_rl = stats.zscore(load_dts(dts_rl_path), axis=0, nan_policy='omit')\n",
    "            dtss_all = np.concatenate([dts_lr, dts_rl]).T# , load_dts(dts2_lr) , load_dts(dts2_rl)]).T\n",
    "            \n",
    "            np.save(output_dir + sbj + '_rfMRI_REST1_LR_RL.npy' , dtss_all)\n",
    "            sbj_IDs.append(sbj)\n",
    "            del files , dtss_all , dts_lr , dts_rl# , dts2_lr , dts2_rl\n",
    "        except:\n",
    "             print('Something went wrong')\n",
    "    np.save(output_dir+'subject_IDs.npy', sbj_IDs)\n",
    "        \n",
    "\n",
    "# Generate the .dscalar image of gradient 1 using brain models from a template file\n",
    "def mk_grad1_dscalar(subject_ID, grad1_npy, template_dscalar, output_dir):\n",
    "\n",
    "    data_length = template_dscalar.shape[1]\n",
    "    data = np.zeros([1,data_length])\n",
    "    data[0,0:grad1_npy.shape[0]] = grad1_npy\n",
    "\n",
    "    ax0 = nib.cifti2.cifti2_axes.ScalarAxis(['Gradient 1'])\n",
    "    ax1 = template_dscalar.header.get_axis(1)\n",
    "    new_img = nib.Cifti2Image(data, header=[ax0, ax1],nifti_header=template_dscalar.nifti_header)\n",
    "    new_img.update_headers()\n",
    "\n",
    "    new_img.to_filename(\"/home/fralberti/Data/Gradient_1/%s_grad1.dscalar.nii\" % subject_ID)\n",
    "    del template_dscalar\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade3d466-7cf5-4a85-8e70-bbbf7afa90ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching folder:101006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching folder:100206\n",
      "Something went wrong\n",
      "searching folder:100408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching folder:100610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching folder:.ipynb_checkpoints\n",
      "Something went wrong\n"
     ]
    }
   ],
   "source": [
    "\n",
    "concat_dts(\"/home/fralberti/Data/HCP_func/\",\"/home/fralberti/Data/Concatenated_dtseries/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f135f45-ea37-4aea-b8e7-143a0b8315d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sbj_IDs = np.load('/home/fralberti/Data/Concatenated_dtseries/subject_IDs.npy')\n",
    "dtseries_list = [np.load('/home/fralberti/Data/Concatenated_dtseries/%s_rfMRI_REST1_LR_RL.npy' % x) for x in sbj_IDs]\n",
    "\n",
    "gcca = embed.GCCA(n_components=4)\n",
    "res = gcca.fit_transform(dtseries_list)\n",
    "del dtseries_list\n",
    "\n",
    "for i,sbj in enumerate(sbj_IDs):\n",
    "    np.save('/home/fralberti/Data/Gradient_1/%s_grad1.npy' % sbj,res[i,:,0])\n",
    "    \n",
    "del res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfeeb6f2-7e06-475a-9865-8a39458f9ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n",
      "pixdim[1,2,3] should be non-zero; setting 0 dims to 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for root, dirs, files in os.walk(\"/home/fralberti/Data/Gradient_1/\"):\n",
    "    npy_files = list(filter(lambda x: re.match('.*\\.npy$', x), files))\n",
    "    for file in npy_files:\n",
    "        subject_ID = file.split(\"_\")[0]\n",
    "        path_to_data = os.path.join(root, file)\n",
    "        grad1_npy = np.load(path_to_data)\n",
    "        template_dscalar = nib.load(\"/home/fralberti/Data/HCP_zone_prim/%s/%s_tfMRI_MOTOR_level2_hp200_s12.dscalar.nii\" % (subject_ID,subject_ID))\n",
    "        output_dir = \"/home/fralberti/Data/Gradient_1/\"\n",
    "        \n",
    "        mk_grad1_dscalar(subject_ID, grad1_npy, template_dscalar, output_dir)\n",
    "        del grad1_npy, template_dscalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6069d6a-d9b2-4f6b-a683-f5e785463c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "!wb_command -cifti-smoothing '/home/fralberti/Data/Gradient_1/100408_grad1.dscalar.nii' \\\n",
    "-fwhm 3 0 COLUMN '/home/fralberti/Data/Gradient_1/100408_grad1_smooth.dscalar.nii' \\\n",
    "-left-surface '/home/fralberti/Data/HCP_zone_prim/100408/100408.L.midthickness.32k_fs_LR.surf.gii' \\\n",
    "-right-surface '/home/fralberti/Data/HCP_zone_prim/100408/100408.L.midthickness.32k_fs_LR.surf.gii'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "868205b8-4fb9-4394-8eda-6188b0fdc422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOOTH A CIFTI FILE\n",
      "   wb_command -cifti-smoothing\n",
      "      <cifti> - the input cifti\n",
      "      <surface-kernel> - the size of the gaussian surface smoothing kernel in\n",
      "         mm, as sigma by default\n",
      "      <volume-kernel> - the size of the gaussian volume smoothing kernel in mm,\n",
      "         as sigma by default\n",
      "      <direction> - which dimension to smooth along, ROW or COLUMN\n",
      "      <cifti-out> - output - the output cifti\n",
      "\n",
      "      [-fwhm] - kernel sizes are FWHM, not sigma\n",
      "\n",
      "      [-left-surface] - specify the left surface to use\n",
      "         <surface> - the left surface file\n",
      "\n",
      "         [-left-corrected-areas] - vertex areas to use instead of computing\n",
      "            them from the left surface\n",
      "            <area-metric> - the corrected vertex areas, as a metric\n",
      "\n",
      "      [-right-surface] - specify the right surface to use\n",
      "         <surface> - the right surface file\n",
      "\n",
      "         [-right-corrected-areas] - vertex areas to use instead of computing\n",
      "            them from the right surface\n",
      "            <area-metric> - the corrected vertex areas, as a metric\n",
      "\n",
      "      [-cerebellum-surface] - specify the cerebellum surface to use\n",
      "         <surface> - the cerebellum surface file\n",
      "\n",
      "         [-cerebellum-corrected-areas] - vertex areas to use instead of\n",
      "            computing them from the cerebellum surface\n",
      "            <area-metric> - the corrected vertex areas, as a metric\n",
      "\n",
      "      [-cifti-roi] - smooth only within regions of interest\n",
      "         <roi-cifti> - the regions to smooth within, as a cifti file\n",
      "\n",
      "      [-fix-zeros-volume] - treat values of zero in the volume as missing data\n",
      "\n",
      "      [-fix-zeros-surface] - treat values of zero on the surface as missing\n",
      "         data\n",
      "\n",
      "      [-merged-volume] - smooth across subcortical structure boundaries\n",
      "\n",
      "      The input cifti file must have a brain models mapping on the chosen\n",
      "      dimension, columns for .dtseries, and either for .dconn.  By default,\n",
      "      data in different structures is smoothed independently (i.e., \"parcel\n",
      "      constrained\" smoothing), so volume structures that touch do not smooth\n",
      "      across this boundary.  Specify -merged-volume to ignore these boundaries.\n",
      "      Surface smoothing uses the GEO_GAUSS_AREA smoothing method.\n",
      "\n",
      "      The -*-corrected-areas options are intended for when it is unavoidable to\n",
      "      smooth on group average surfaces, it is only an approximate correction\n",
      "      for the reduction of structure in a group average surface.  It is better\n",
      "      to smooth the data on individuals before averaging, when feasible.\n",
      "\n",
      "      The -fix-zeros-* options will treat values of zero as lack of data, and\n",
      "      not use that value when generating the smoothed values, but will fill\n",
      "      zeros with extrapolated values.  The ROI should have a brain models\n",
      "      mapping along columns, exactly matching the mapping of the chosen\n",
      "      direction in the input file.  Data outside the ROI is ignored.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wb_command -cifti-smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deeea06-8b5c-4ac1-af87-dcdf0eac73ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
